{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gan.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNsb1IE6YIJxTNFrYLqUIKu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AxmYuVKW_ime","executionInfo":{"status":"ok","timestamp":1607743777741,"user_tz":360,"elapsed":25849,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}},"outputId":"7feefc0b-877c-4941-feba-580217a7b63d"},"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3ERXj_V_m8z","executionInfo":{"status":"ok","timestamp":1607743781091,"user_tz":360,"elapsed":665,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}},"outputId":"744bf98a-83e1-45bf-fba5-7bdf34da8e53"},"source":["%cd gdrive/'My Drive'/gan_results/"],"execution_count":22,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/gan_results\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0cHif6PBPfEO"},"source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 64\n","\n","# convert data to torch.FloatTensor\n","transform = transforms.ToTensor()\n","\n","# get the training datasets\n","train_data = datasets.MNIST(root='data', train=True,\n","                                   download=True, transform=transform)\n","\n","# prepare data loader\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","                                           num_workers=num_workers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsGG7yx1PwQK"},"source":["# obtain one batch of training images\n","dataiter = iter(train_loader)\n","dataiter.next()\n","images, labels = dataiter.next()\n","labels\n","images\n","images.numpy()\n","images = images.numpy()\n","images[0]\n","np.squeeze(images[0])\n","# # get one image from the batch\n","# img = np.squeeze(images[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3P-VTjVP8E-","executionInfo":{"status":"ok","timestamp":1607740590563,"user_tz":360,"elapsed":1236,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Discriminator(nn.Module):\n","\n","    def __init__(self, input_size, hidden_dim, output_size):\n","        super(Discriminator, self).__init__()\n","        \n","        # define all layers\n","        self.fc1=nn.Linear(input_size,hidden_dim*4)\n","        self.fc2=nn.Linear(hidden_dim*4,hidden_dim*2)\n","        self.fc3=nn.Linear(hidden_dim*2,hidden_dim)\n","        self.fc4=nn.Linear(hidden_dim,output_size)\n","        \n","        self.droppout=nn.Dropout(0.3)\n","        \n","        \n","    def forward(self, x):\n","        # flatten image\n","        x=x.view(-1,28*28)\n","        \n","        # pass x through all layers\n","        x=self.fc1(x)\n","        x=F.leaky_relu(x,0.2)\n","        x=self.droppout(x)\n","        \n","        x=self.fc2(x)\n","        x=F.leaky_relu(x,0.2)\n","        x=self.droppout(x)\n","        \n","        x=self.fc3(x)\n","        x=F.leaky_relu(x,0.2)\n","        x=self.droppout(x)\n","        \n","        out=self.fc4(x)\n","        \n","\n","        \n","        # apply leaky relu activation to all hidden layers\n","\n","        return out"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fLO9uDiQAdU","executionInfo":{"status":"ok","timestamp":1607740685235,"user_tz":360,"elapsed":592,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}}},"source":["class Generator(nn.Module):\n","\n","    def __init__(self, input_size, hidden_dim, output_size):\n","        super(Generator, self).__init__()\n","        \n","        # define all layers\n","        self.fc1=nn.Linear(input_size,hidden_dim)\n","        self.fc2=nn.Linear(hidden_dim,hidden_dim*2)\n","        self.fc3=nn.Linear(hidden_dim*2,hidden_dim*4)\n","        self.fc4=nn.Linear(hidden_dim*4,output_size)\n","        self.droppout=nn.Dropout(0.3)\n","        \n","\n","    def forward(self, x):\n","        # pass x through all layers\n","        x=self.fc1(x)\n","        x=F.leaky_relu(x,0.2)\n","        x=self.droppout(x)\n","        x=self.fc2(x)\n","        x=F.leaky_relu(x,0.2)\n","        x=self.droppout(x)\n","        x=self.fc3(x)\n","        x=F.leaky_relu(x,0.2)\n","        x=self.droppout(x)\n","        x=self.fc4(x)\n","        x=F.tanh(x)\n","        \n","        # final layer should have tanh applied\n","        \n","        return x"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcCuGqSqQE5T","executionInfo":{"status":"ok","timestamp":1607740676051,"user_tz":360,"elapsed":617,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}}},"source":["# Discriminator hyperparams\n","\n","# Size of input image to discriminator (28*28)\n","input_size = 784\n","# Size of discriminator output (real or fake)\n","d_output_size = 1\n","# Size of *last* hidden layer in the discriminator\n","d_hidden_size = 32\n","\n","# Generator hyperparams\n","\n","# Size of latent vector to give to generator\n","z_size = 200\n","# Size of discriminator output (generated image)\n","g_output_size = 784\n","# Size of *first* hidden layer in the generator\n","g_hidden_size = 32"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"06xYbqk-QJCV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607740975284,"user_tz":360,"elapsed":669,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}},"outputId":"267adc48-1393-4d65-dd36-5e00914efc5a"},"source":["from torchsummary import summary\n","# instantiate discriminator and generator\n","D = Discriminator(input_size, d_hidden_size, d_output_size)\n","G = Generator(z_size, g_hidden_size, g_output_size)\n","\n","# check that they are as you expect\n","summary(D, (784, 128))\n","summary(G, (32, 200))\n","print(D)\n","print()\n","print(G)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                  [-1, 128]         100,480\n","           Dropout-2                  [-1, 128]               0\n","            Linear-3                   [-1, 64]           8,256\n","           Dropout-4                   [-1, 64]               0\n","            Linear-5                   [-1, 32]           2,080\n","           Dropout-6                   [-1, 32]               0\n","            Linear-7                    [-1, 1]              33\n","================================================================\n","Total params: 110,849\n","Trainable params: 110,849\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.38\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.42\n","Estimated Total Size (MB): 0.81\n","----------------------------------------------------------------\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1               [-1, 32, 32]           6,432\n","           Dropout-2               [-1, 32, 32]               0\n","            Linear-3               [-1, 32, 64]           2,112\n","           Dropout-4               [-1, 32, 64]               0\n","            Linear-5              [-1, 32, 128]           8,320\n","           Dropout-6              [-1, 32, 128]               0\n","            Linear-7              [-1, 32, 784]         101,136\n","================================================================\n","Total params: 118,000\n","Trainable params: 118,000\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.02\n","Forward/backward pass size (MB): 0.30\n","Params size (MB): 0.45\n","Estimated Total Size (MB): 0.78\n","----------------------------------------------------------------\n","Discriminator(\n","  (fc1): Linear(in_features=784, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n","  (droppout): Dropout(p=0.3, inplace=False)\n",")\n","\n","Generator(\n","  (fc1): Linear(in_features=200, out_features=32, bias=True)\n","  (fc2): Linear(in_features=32, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=128, bias=True)\n","  (fc4): Linear(in_features=128, out_features=784, bias=True)\n","  (droppout): Dropout(p=0.3, inplace=False)\n",")\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"i8tuoh7lRop-","executionInfo":{"status":"ok","timestamp":1607741482377,"user_tz":360,"elapsed":683,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}}},"source":["\n","# Calculate losses\n","def real_loss(D_out, smooth=False):\n","    # compare logits to real labels\n","    # smooth labels if smooth=True\n","    size=D_out.size(0)\n","    if smooth:\n","        labels=torch.ones(size)*0.9\n","    else:\n","        labels=torch.ones(size)\n","\n","    criterion=nn.BCEWithLogitsLoss()\n","    loss = criterion(D_out.squeeze(),labels)\n","    return loss\n","\n","def fake_loss(D_out):\n","    # compare logits to fake labels\n","    size=D_out.size(0)\n","    labels=torch.zeros(size)\n","    criterion=nn.BCEWithLogitsLoss()\n","    loss = criterion(D_out.squeeze(),labels)\n","    return loss"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTm4cfe-RrdV","executionInfo":{"status":"ok","timestamp":1607741487731,"user_tz":360,"elapsed":678,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}}},"source":["import torch.optim as optim\n","\n","# learning rate for optimizers\n","lr = 0.002\n","\n","# Create optimizers for the discriminator and generator\n","d_optimizer = optim.Adam(D.parameters(),lr=lr)\n","g_optimizer = optim.Adam(G.parameters(),lr=lr)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"f25djn9HRyqk"},"source":["import pickle as pkl\n","# training hyperparams\n","num_epochs = 1000\n","\n","# keep track of loss and generated, \"fake\" samples\n","samples = []\n","losses = []\n","\n","print_every = 400\n","\n","# Get some fixed data for sampling. These are images that are held\n","# constant throughout training, and allow us to inspect the model's performance\n","sample_size=16\n","fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n","fixed_z = torch.from_numpy(fixed_z).float()\n","\n","# train the network\n","D.train()\n","G.train()\n","for epoch in range(num_epochs):\n","    \n","    for batch_i, (real_images, _) in enumerate(train_loader):\n","                \n","        batch_size = real_images.size(0)\n","        \n","        ## Important rescaling step ## \n","        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n","        \n","        # ============================================\n","        #            TRAIN THE DISCRIMINATOR\n","        # ============================================\n","                \n","        # 1. Train with real images\n","\n","        # Compute the discriminator losses on real images\n","        # use smoothed labels\n","        \n","        d_optimizer.zero_grad()\n","        D_real=D(real_images)\n","        \n","        d_realloss=real_loss(D_real,smooth=True)\n","        \n","        # 2. Train with fake images\n","        \n","        \n","        # Generate fake images\n","        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n","        z = torch.from_numpy(z).float()\n","        fake_images = G(z)\n","        \n","        # Compute the discriminator losses on fake images\n","        D_fake=D(fake_images)\n","        d_fakeloss=fake_loss(D_fake)\n","        \n","        # add up real and fake losses and perform backprop\n","        d_loss = d_realloss+d_fakeloss\n","        d_loss.backward()\n","        d_optimizer.step()\n","        \n","        \n","        # =========================================\n","        #            TRAIN THE GENERATOR\n","        # =========================================\n","        g_optimizer.zero_grad()\n","        \n","        # 1. Train with fake images and flipped labels\n","        \n","        # Generate fake images\n","        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n","        z = torch.from_numpy(z).float()\n","        G_fake=G(z)\n","        \n","        # Compute the discriminator losses on fake images \n","        # using flipped labels!\n","        DG_fake=D(G_fake)\n","        # perform backprop\n","        \n","        G_fakeloss=real_loss(DG_fake)\n","        g_loss=G_fakeloss\n","        g_loss.backward()\n","        g_optimizer.step()\n","        \n","        \n","        \n","        \n","        \n","\n","        # Print some loss stats\n","        if batch_i % print_every == 0:\n","            # print discriminator and generator loss\n","            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n","                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n","\n","    \n","    ## AFTER EACH EPOCH##\n","    # append discriminator loss and generator loss\n","    losses.append((d_loss.item(), g_loss.item()))\n","    \n","    # generate and save sample, fake images\n","    G.eval() # eval mode for generating samples\n","    samples_z = G(fixed_z)\n","    samples.append(samples_z)\n","    G.train() # back to train mode\n","\n","\n","# Save training generator samples\n","with open('train_samples.pkl', 'wb') as f:\n","    pkl.dump(samples, f)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVjEVgXDcOHs"},"source":["fig, ax = plt.subplots()\n","losses = np.array(losses)\n","plt.plot(losses.T[0], label='Discriminator')\n","plt.plot(losses.T[1], label='Generator')\n","plt.title(\"Training Losses\")\n","plt.legend()\n","import pickle as pkl\n","with open('train_samples.pkl', 'wb') as f:\n","    pkl.dump(samples, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WeRAnwp-5CU","executionInfo":{"status":"ok","timestamp":1607750480553,"user_tz":360,"elapsed":807,"user":{"displayName":"Chandan Chowdary","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyNjrLdUjEAEA91Qy62CQcKmxXNIDOt4hUj7RqDQ=s64","userId":"05175208831610953878"}}},"source":["from torchvision.utils import save_image\r\n","\r\n","def save_images_to_colab(img, i):\r\n","  save_image(img.reshape((28,28)), 'gan_'+i+ '.png')"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"2seKVI7YKMHt"},"source":["rows = 10 # split epochs into 10, so 100/10 = every 10 epochs\n","cols = 4\n","fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n","count=0\n","print(len(samples))\n","for sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\n","    print(count)\n","    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n","        img = img.detach()\n","        count+=1\n","        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n","        #save_images_to_colab(img, str(count))\n","        ax.xaxis.set_visible(False)\n","        ax.yaxis.set_visible(False)        \n","        "],"execution_count":null,"outputs":[]}]}